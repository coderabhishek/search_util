If we can invert an n × n matrix in time I (n), where I (n) = Ω (n2) and I (n) satisfies the
regularity condition I (3n) = O(I (n)), then we can multiply two n ×n matrices in time O(I (n)).

Proof Let A and B be n × n matrices whose matrix product C we wish to compute. We define
the 3n × 3n matrix D by

The inverse of D is

and thus we can compute the product AB by taking the upper right n × n submatrix of D-1.
We can construct matrix D in Θ(n2) = O(I (n)) time, and we can invert D in O(I (3n)) = O(I
(n)) time, by the regularity condition on I(n). We thus have M(n) = O(I (n)).
Note that I (n) satisfies the regularity condition whenever I (n) = Θ(nc lgd n) for any constants
c > 0 and d ≥ 0.
The proof that matrix inversion is no harder than matrix multiplication relies on some
properties of symmetric positive-definite matrices that will be proved in Section 28.5.
Theorem 28.8: (Inversion is no harder than multiplication)
Suppose we can multiply two n × n real matrices in time M(n), where M(n) = Ω(n2) and M(n)
satisfies the two regularity conditions M(n + k) = O(M(n)) for any k in the range 0 ≤ k ≤ n and
M(n/2) ≤ cM(n) for some constant c < 1/2. Then we can compute the inverse of any real
nonsingular n × n matrix in time O(M(n)).
Proof We can assume that n is an exact power of 2, since we have

for any k > 0. Thus, by choosing k such that n + k is a power of 2, we enlarge the matrix to a
size that is the next power of 2 and obtain the desired answer A-1 from the answer to the
enlarged problem. The first regularity condition on M(n) ensures that this enlargement does
not cause the running time to increase by more than a constant factor.

