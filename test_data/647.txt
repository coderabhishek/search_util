The latter two reasons were mitigated around 1990. Higham [145] demonstrated that the
difference in numerical stability had been overemphasized; although Strassen's algorithm is
too numerically unstable for some applications, it is within acceptable limits for others. Bailey
et al. [30] discuss techniques for reducing the memory requirements for Strassen's algorithm.
In practice, fast matrix-multiplication implementations for dense matrices use Strassen's
algorithm for matrix sizes above a "crossover point," and they switch to the naive method
once the subproblem size reduces to below the crossover point. The exact value of the
crossover point is highly system dependent. Analyses that count operations but ignore effects
from caches and pipelining have produced crossover points as low as n = 8 (by Higham [145])
or n = 12 (by Huss-Lederman et al. [163]). Empirical measurements typically yield higher
crossover points, with some as low as n = 20 or so. For any given system, it is usually
straightforward to determine the crossover point by experimentation.
By using advanced techniques beyond the scope of this text, one can in fact multiply n × n
matrices in better than Θ(nlg 7) time. The current best upper bound is approximately O(n2.376).
The best lower bound known is just the obvious Ω(n2) bound (obvious because we have to fill
in n2 elements of the product matrix). Thus, we currently do not know exactly how hard
matrix multiplication really is.
Exercises 28.2-1
Use Strassen's algorithm to compute the matrix product

Show your work.

Exercises 28.2-2
How would you modify Strassen's algorithm to multiply n × n matrices in which n is not an
exact power of 2? Show that the resulting algorithm runs in time Θ(nlg 7).

Exercises 28.2-3
What is the largest k such that if you can multiply 3 × 3 matrices using k multiplications (not
assuming commutativity of multiplication), then you can multiply n × n matrices in time o(nlg
7
)? What would the running time of this algorithm be?

Exercises 28.2-4

