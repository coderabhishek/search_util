Optimal substructure varies across problem domains in two ways:
1. how many subproblems are used in an optimal solution to the original problem, and
2. how many choices we have in determining which subproblem(s) to use in an optimal
solution.
In assembly-line scheduling, an optimal solution uses just one subproblem, but we must
consider two choices in order to determine an optimal solution. To find the fastest way
through station Si,j , we use either the fastest way through S1, j -1 or the fastest way through S2, j
-1; whichever we use represents the one subproblem that we must optimally solve. Matrixchain multiplication for the subchain Ai Ai+1 Aj serves as an example with two subproblems
and j - i choices. For a given matrix Ak at which we split the product, we have two
subproblems—parenthesizing Ai Ai+1 Ak and parenthesizing Ak+1 Ak+2 Aj—and we must solve
both of them optimally. Once we determine the optimal solutions to subproblems, we choose
from among j - i candidates for the index k.
Informally, the running time of a dynamic-programming algorithm depends on the product of
two factors: the number of subproblems overall and how many choices we look at for each
subproblem. In assembly-line scheduling, we had Θ(n) subproblems overall, and only two
choices to examine for each, yielding a Θ(n) running time. For matrix-chain multiplication,
there were Θ(n2) subproblems overall, and in each we had at most n - 1 choices, giving an
O(n3) running time.
Dynamic programming uses optimal substructure in a bottom-up fashion. That is, we first find
optimal solutions to subproblems and, having solved the subproblems, we find an optimal
solution to the problem. Finding an optimal solution to the problem entails making a choice
among subproblems as to which we will use in solving the problem. The cost of the problem
solution is usually the subproblem costs plus a cost that is directly attributable to the choice
itself. In assembly-line scheduling, for example, first we solved the subproblems of finding
the fastest way through stations S1, j -1 and S2, j -1, and then we chose one of these stations as the
one preceding station Si, j. The cost attributable to the choice itself depends on whether we
switch lines between stations j - 1 and j; this cost is ai, j if we stay on the same line, and it is ti′,
j-1 + ai,j , where i′ ≠ i, if we switch. In matrix-chain multiplication, we determined optimal
parenthesizations of subchains of Ai Ai+1 Aj , and then we chose the matrix Ak at which to split
the product. The cost attributable to the choice itself is the term pi-1 pk pj.
In Chapter 16, we shall examine "greedy algorithms," which have many similarities to
dynamic programming. In particular, problems to which greedy algorithms apply have
optimal substructure. One salient difference between greedy algorithms and dynamic
programming is that in greedy algorithms, we use optimal substructure in a top-down fashion.
Instead of first finding optimal solutions to subproblems and then making a choice, greedy
algorithms first make a choice—the choice that looks best at the time—and then solve a
resulting subproblem.
Subtleties

One should be careful not to assume that optimal substructure applies when it does not.
Consider the following two problems in which we are given a directed graph G = (V, E) and
vertices u, v V.

