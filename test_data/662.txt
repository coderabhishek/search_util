For the moment, let us assume that the n × n matrix A is symmetric and positive-definite. We
partition A into four n/2 × n/2 submatrices:
(28.25)
Then, if we let
(28.26)
be the Schur complement of A with respect to B (we shall see more about this form of Schur
complement in Section 28.5), we have
(28.27)
since AA-1 = In, as can be verified by performing the matrix multiplication. The matrices B-1
and S-1 exist if A is symmetric and positive-definite, by Lemmas 28.9, 28.10, and 28.11 in
Section 28.5, because both B and S are symmetric and positive-definite. By Exercise 28.1-2,
B-1CT = (C B-1)T and B-1CTS-1 = (S1C B-1)T. Equations (28.26) and (28.27) can therefore be
used to specify a recursive algorithm involving four multiplications of n/2 × n/2 matrices:
C · B-1,
(C B-1) · CT,
S-1 · (C B-1),
(C B-1)T · (S-1C B-1).
Thus, we can invert an n × n symmetric positive-definite matrix by inverting two n/2 × n/2
matrices (B and S), performing these four multiplications of n/2 × n/2 matrices (which we can
do with an algorithm for n × n matrices), plus an additional cost of O(n2) for extracting
submatrices from A and performing a constant number of additions and subtractions on these
n/2 × n/2 matrices. We get the recurrence
I (n) ≤ 2I(n/2) + 4M(n) + O(n2)
= 2I(n/2) + Θ (M(n))
= O(M(n)).
The second line holds because M(n) = Ω(n2), and the third line follows because the second
regularity condition in the statement of the theorem allows us to apply case 3 of the master
theorem (Theorem 4.1).
It remains to prove that the asymptotic running time of matrix multiplication can be obtained
for matrix inversion when A is invertible but not symmetric and positive-definite. The basic
idea is that for any nonsingular matrix A, the matrix AT A is symmetric (by Exercise 28.1-2)
and positive-definite (by Theorem 28.6). The trick, then, is to reduce the problem of inverting
A to the problem of inverting AT A.

